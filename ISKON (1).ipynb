{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUqShJS2Lio0",
        "outputId": "3922b651-15e6-44fc-f812-b8fe1fc3ccce"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 KB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993243 sha256=f210675d9d5b634a1a5383019652c5a4a59c1bc29319cdb5f94449e5534c6232\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/c1/d9/7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Q4VQis_fIFd5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langdetect import detect\n",
        "import re\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_language(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "    except:\n",
        "        lang = 'unknown'\n",
        "    return lang"
      ],
      "metadata": {
        "id": "LqeAj4YwLokI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_excel('/content/ISKCON Data.xlsx')"
      ],
      "metadata": {
        "id": "6kikqBUYIuVD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['new_text'] = df['REVIEW SUBJECT'].fillna(\" \") + \" \" + df[\"text\"].fillna(\" \")\n",
        "df['new_text'].str.strip()\n",
        "df['length'] = df['new_text'].str.split().str.len()\n",
        "df = df[df['length'] > 3]"
      ],
      "metadata": {
        "id": "n08SQX1JNlU9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = r'[^a-zA-Z\\s]'\n",
        "\n",
        "# define a function to remove non-alphabetic characters from a text string\n",
        "def remove_non_alpha(text):\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# apply the function to the column you want to clean\n",
        "df['text_lang'] = df['new_text'].apply(remove_non_alpha)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haCfjFb0Q8po",
        "outputId": "595efe1b-a18d-46b4-b1df-908f8d047caf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-b73e790a1738>:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text_lang'] = df['new_text'].apply(remove_non_alpha)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_short_words(text):\n",
        "    words = text.split()\n",
        "    words = [word for word in words if len(word) >= 2]\n",
        "    return ' '.join(words)\n",
        "\n",
        "# apply the function to the column you want to clean\n",
        "df['text_lang'] = df['text_lang'].apply(lambda x: remove_short_words(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYM4SEogQI1c",
        "outputId": "0d28f061-20c8-4dcf-8daf-c4b44e47b2fc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-4bb6120c96b9>:7: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['text_lang'] = df['text_lang'].apply(lambda x: remove_short_words(x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['language'] = df['text_lang'].apply(detect_language)"
      ],
      "metadata": {
        "id": "1RSiSgwGJZUq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "saujnD71aKEK",
        "outputId": "e1bf375b-9091-44fe-83c4-8d459fabbd5c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sia = SentimentIntensityAnalyzer()\n",
        "def sentiment_scores(text):\n",
        "    pos_sent = []\n",
        "    neg_sent = []\n",
        "    sentences = nltk.sent_tokenize(text)\n",
        "    pos_count = 0\n",
        "    neg_count = 0\n",
        "    for sentence in sentences:\n",
        "        scores = sia.polarity_scores(sentence)\n",
        "        if scores['compound'] > 0:\n",
        "            pos_count += 1\n",
        "            pos_sent.append(sentence)\n",
        "        elif scores['compound'] < 0:\n",
        "            neg_count += 1\n",
        "            neg_sent.append(sentence)\n",
        "    total_count = pos_count + neg_count\n",
        "    if total_count > 0:\n",
        "        pos_pct = pos_count / total_count * 100\n",
        "        neg_pct = neg_count / total_count * 100\n",
        "    else:\n",
        "        pos_pct = 0\n",
        "        neg_pct = 0\n",
        "    return [pos_sent,pos_pct,neg_sent,neg_pct]"
      ],
      "metadata": {
        "id": "Dlls8xg9bt7y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHWdTetscfCA",
        "outputId": "b460d12f-6525-45ea-b372-76f9b670abf6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Sentiment'] = df['new_text'].apply(sentiment_scores)"
      ],
      "metadata": {
        "id": "6s6Y3GQ3bx8U"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[['pos_sent','pos','neg_sent','neg']] = df['Sentiment'].apply(lambda x: pd.Series(x))\n",
        "\n",
        "# drop the original list_column column\n",
        "df.drop('Sentiment', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "1gnQd4N_fOl-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NRCLex"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpLD_rk6gO8x",
        "outputId": "e30415a4-ab6d-4dff-f967-23d910675dac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting NRCLex\n",
            "  Downloading NRCLex-4.0-py3-none-any.whl (4.4 kB)\n",
            "  Downloading NRCLex-3.0.0.tar.gz (396 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.9/dist-packages (from NRCLex) (0.17.1)\n",
            "Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.9/dist-packages (from textblob->NRCLex) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (4.65.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (1.1.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.9/dist-packages (from nltk>=3.1->textblob->NRCLex) (2022.10.31)\n",
            "Building wheels for collected packages: NRCLex\n",
            "  Building wheel for NRCLex (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for NRCLex: filename=NRCLex-3.0.0-py3-none-any.whl size=43327 sha256=9df40d35f45371887ebb63235a4338f43cc2a559922b5cc2af8fd07c0fa75313\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/c4/f2/c390dd3eac398fdf45f7a01c6516bc53fa7a9ab59c7d2ff518\n",
            "Successfully built NRCLex\n",
            "Installing collected packages: NRCLex\n",
            "Successfully installed NRCLex-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nrclex import NRCLex"
      ],
      "metadata": {
        "id": "m_QuEXXmhsLr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['emotions'] = df.new_text.apply(lambda x: NRCLex(x).raw_emotion_scores)"
      ],
      "metadata": {
        "id": "B6lnuGeGiLfA"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('final.csv')"
      ],
      "metadata": {
        "id": "Y2wo8X32jX-k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['emotions']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UFsbyhtWClC",
        "outputId": "8123eae3-bae3-44d9-d9b0-8082beae73cb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0                               {'positive': 6, 'joy': 1}\n",
              "1       {'positive': 4, 'trust': 3, 'anticipation': 4,...\n",
              "2       {'positive': 10, 'trust': 8, 'joy': 6, 'anger'...\n",
              "3                          {'negative': 1, 'positive': 1}\n",
              "4       {'anticipation': 3, 'joy': 6, 'positive': 6, '...\n",
              "                              ...                        \n",
              "4642                {'joy': 1, 'positive': 1, 'trust': 1}\n",
              "4643    {'anticipation': 1, 'joy': 2, 'positive': 3, '...\n",
              "4646    {'joy': 2, 'positive': 4, 'surprise': 1, 'trus...\n",
              "4649    {'positive': 2, 'anger': 1, 'negative': 1, 'an...\n",
              "4650                            {'joy': 1, 'positive': 1}\n",
              "Name: emotions, Length: 4274, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "positive = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    positive = positive + i['positive'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "nY5O-6AYAGL6"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "negative = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    negative = negative + i['negative'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "WCBglGsWWoGv"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joy = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    joy = joy + i['joy'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "SCU5aCrJXT96"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trust = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    trust = trust + i['trust'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "IIGwA4kaXcmq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anticipation = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    anticipation = anticipation + i['anticipation'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "ldcq7JdvXqwo"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "surprise = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    surprise = surprise + i['surprise'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "V9Wm4wC3X9Un"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anger = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    anger = anger + i['anger'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "gsjBj3-bYRjE"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sadness = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    sadness = sadness + i['sadness'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "0vmZRjmvZGqu"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disgust = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    disgust = disgust + i['disgust'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "k0WvZnKqZ6jp"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fear = 0\n",
        "for i in df['emotions']:\n",
        "  try:\n",
        "    fear = fear + i['fear'] \n",
        "  except:\n",
        "    continue"
      ],
      "metadata": {
        "id": "2mhogK_caFmr"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame([fear, anger, anticipation, trust, surprise, positive, negative, sadness, disgust, joy],['fear', 'anger', 'anticipation', 'trust', 'surprise', 'positive', 'negative', 'sadness', 'disgust', 'joy']).to_csv('emotions.csv')"
      ],
      "metadata": {
        "id": "VPqzII0Fawur"
      },
      "execution_count": 69,
      "outputs": []
    }
  ]
}